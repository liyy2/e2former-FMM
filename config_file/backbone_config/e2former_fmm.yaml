# Node-only E2Former variant backed by FMM attention.
encoder: "default" # or dit
with_cluster: false
use_rdkit: false
# dit_config
encoder_embed_dim: 1024
ffn_embedding_dim: 1536
num_attention_heads: 32
dropout: 0.1
num_encoder_layers: 12


# global_cfg:
regress_forces: true
direct_force: false
use_padding: true
use_fp16_backbone: false
use_compile: false

flatten_atoms_threshold: 1000

pbc_expanded_token_cutoff: 256
pbc_expanded_num_cell_per_direction: 4


# backbone_config:
max_neighbors: 20
irreps_node_embedding: "128x0e+128x1e+128x2e"
num_layers: 8
pbc_max_radius: 15.0
max_radius: 15.0
basis_type: "gaussiansmear"
number_of_basis: 128
num_attn_heads: 4
attn_scalar_head: 32
irreps_head: "32x0e+32x1e+32x2e"
rescale_degree: False
nonlinear_message: False
norm_layer: "rms_norm_sh_BL"
alpha_drop: 0.1
proj_drop: 0.0
out_drop: 0.0
drop_path_rate: 0.1
attn_type: "fmm-node"
tp_type : "fmm-node+tp_cueq"
fmm_num_kappa: 8
fmm_kappa_min: 1.0
fmm_kappa_max: 1.4
# FMM speed/accuracy knobs
fmm_num_directions: 25
fmm_kappa_chunk_size: 0
fmm_compute_dtype: "auto" # auto|fp32|bf16|fp16
# Optional per-head V bottleneck for FMM (0 = disable; e.g., 8 or 16 for speed).
fmm_value_head_dim: 0
edge_embedtype: "eqv2"
attn_biastype: "share"
ffn_type: "eqv2ffn"
add_rope: False
time_embed: False
sparse_attn: False
dynamic_sparse_attn_threthod: 1000
force_head: null
